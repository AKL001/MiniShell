1-Word tokens:
    Keep reading until you hit a special character or whitespace
    Treat these as commands or arguments

2-Handling Edge Cases

    Escape characters:

    If a character is preceded by a backslash, treat it literally
    Example: echo Hello\ World should be one token, not two


    Environment variables:

    You'll need to expand these during or after tokenization
    Example: echo $HOME should expand to the home directory


    Command substitution:

    Handle $(command) or backticks by recursively executing the inner command

3-Testing Your Tokenizer
    Test with increasingly complex commands:

    echo hello
    ls -la | grep .c
    cat file > output.txt
    echo "quoted text with spaces"
    ls && echo success || echo failure

4-Input: "echo "Hello World" | grep Hello > output.txt && ls -la"

    Tokens and their types:
    1. "echo"          → T_WORD
    2. "Hello World"   → T_WORD (is_quoted=1)
    3. "|"             → T_PIPE
    4. "grep"          → T_WORD
    5. "Hello"         → T_WORD
    6. ">"             → T_REDIR_OUT
    7. "output.txt"    → T_WORD
    8. "&&"            → T_AND
    9. "ls"            → T_WORD
    10. "-la"          → 
    

5-The strategy for your shell:
    "is to have the parent process do all the piping and redirection 
    before forking the processes. In this way the children will inherit the redirection. The parent 
    needs to save input/output and restore it at the end. Stderr is the same for all processes" 


